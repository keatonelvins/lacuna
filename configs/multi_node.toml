[model]
name = "meta-llama/Llama-3.2-3B"
attention = "SDPA"
compile_mode = "max-autotune"
cce = true

[dist]
backend = "fsdp"
cpu_offload = false  # Enable if OOM

[ac]
mode = "partial"  # Activation checkpointing for memory

[data]
dataset_name = "HuggingFaceFW/fineweb-edu"
seq_len = 4096

[trainer]
batch_size = 256  # Global batch size across all nodes
steps = 10000

[optimizer]
lr = 2e-4
weight_decay = 0.01

[scheduler]
type = "wsd"
warmup_steps = 500
decay_steps = 500

[checkpoint]
save_every = 1000
save_dir = "weights/multi_node"

[torchrun]
nproc_per_node = 8  # GPUs per node
nnodes = 2  # Total number of nodes
master_addr = "10.0.0.1"  # Replace with master node IP
master_port = "29500"
